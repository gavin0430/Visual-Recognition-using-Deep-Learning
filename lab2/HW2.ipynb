{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images8\", exist_ok=True)\n",
    "n_epochs = 200\n",
    "batch_size = 64\n",
    "lr_g = 0.0001\n",
    "lr_d = 0.0004\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "latent_dim = 128\n",
    "channels = 3\n",
    "n_critic = 5\n",
    "sample_interval = 400\n",
    "img_shape = (3, 56, 56)\n",
    "img_size = 28\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 202599\n",
      "    Root location: .\\data\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               CenterCrop(size=(138, 138))\n",
      "               Resize(size=56, interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose([\n",
    "                               transforms.CenterCrop(138), \n",
    "                               transforms.Resize(56),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "training_data = datasets.ImageFolder(root='.\\data',transform=data_transform)\n",
    "print(training_data)\n",
    "dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class Reshape1(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 64, 7, 7)\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return torch.mean(y_true * y_pred)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "              \n",
    "            nn.Linear(latent_dim, 3136, bias=False),\n",
    "            nn.BatchNorm1d(num_features=3136),\n",
    "            nn.LeakyReLU(inplace=True, negative_slope=0.0001),\n",
    "            Reshape1(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=(3, 3), stride=(2, 2), padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.LeakyReLU(inplace=True, negative_slope=0.0001),\n",
    "            #nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=(3, 3), stride=(2, 2), padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.LeakyReLU(inplace=True, negative_slope=0.0001),\n",
    "            #nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=(3, 3), stride=(1, 1), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.LeakyReLU(inplace=True, negative_slope=0.0001),\n",
    "            #nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=(2, 2), stride=(1, 1), padding=0, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, padding=1, kernel_size=(3, 3), stride=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.LeakyReLU(inplace=True, negative_slope=0.0001), \n",
    "            #nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=8, out_channels=16, padding=1, kernel_size=(3, 3), stride=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.LeakyReLU(inplace=True, negative_slope=0.0001), \n",
    "            #nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, padding=1, kernel_size=(3, 3), stride=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.LeakyReLU(inplace=True, negative_slope=0.0001), \n",
    "            #nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            Flatten(),\n",
    "\n",
    "            \n",
    "            #nn.Sigmoid()\n",
    "            )\n",
    "        self.Li = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        #print(validity.shape)\n",
    "        validity = self.Li(validity)\n",
    "        #print(validity.shape)\n",
    "        return validity\n",
    "    \n",
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    \n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Linear(in_features=1024, out_features=9408, bias=True)\n",
      "    (12): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=9408, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, kernel_size = (4,4), stride=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size = (4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size = (4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size = (4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "    \n",
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=lr_g)\n",
    "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=lr_d)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "def adjust_learning_rate(optimizer, epoch, lr_in):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 15 epochs\"\"\"\n",
    "    lr_ =  lr_in * (0.1 ** (epoch // 15))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/3166] [D loss: 9.682660] [G loss: 0.033586]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yi Feng\\Anaconda3\\envs\\dl\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save at loss = 0.033586\n",
      "[Epoch 0/200] [Batch 5/3166] [D loss: -23.463995] [G loss: 0.185594]\n",
      "[Epoch 0/200] [Batch 10/3166] [D loss: -46.913036] [G loss: 0.475544]\n",
      "[Epoch 0/200] [Batch 15/3166] [D loss: -46.136894] [G loss: 1.685362]\n",
      "[Epoch 0/200] [Batch 20/3166] [D loss: -59.334358] [G loss: 0.921033]\n",
      "[Epoch 0/200] [Batch 25/3166] [D loss: -69.302505] [G loss: 0.034190]\n",
      "[Epoch 0/200] [Batch 30/3166] [D loss: -71.131973] [G loss: -0.331753]\n",
      "[Epoch 0/200] [Batch 35/3166] [D loss: -65.500313] [G loss: -1.034641]\n",
      "[Epoch 0/200] [Batch 40/3166] [D loss: -69.330200] [G loss: -1.129175]\n",
      "[Epoch 0/200] [Batch 45/3166] [D loss: -66.217155] [G loss: -1.252615]\n",
      "[Epoch 0/200] [Batch 50/3166] [D loss: -71.282394] [G loss: -1.433748]\n",
      "[Epoch 0/200] [Batch 55/3166] [D loss: -72.225113] [G loss: -1.811457]\n",
      "[Epoch 0/200] [Batch 60/3166] [D loss: -83.373154] [G loss: -2.133715]\n",
      "[Epoch 0/200] [Batch 65/3166] [D loss: -86.545197] [G loss: -2.927700]\n",
      "[Epoch 0/200] [Batch 70/3166] [D loss: -90.849213] [G loss: -2.742267]\n",
      "[Epoch 0/200] [Batch 75/3166] [D loss: -84.117233] [G loss: -2.936418]\n",
      "[Epoch 0/200] [Batch 80/3166] [D loss: -88.397217] [G loss: -2.924319]\n",
      "[Epoch 0/200] [Batch 85/3166] [D loss: -85.971237] [G loss: -3.339779]\n",
      "[Epoch 0/200] [Batch 90/3166] [D loss: -88.845833] [G loss: -3.789998]\n",
      "[Epoch 0/200] [Batch 95/3166] [D loss: -89.638405] [G loss: -4.945680]\n",
      "[Epoch 0/200] [Batch 100/3166] [D loss: -83.055115] [G loss: -4.634538]\n",
      "[Epoch 0/200] [Batch 105/3166] [D loss: -87.094887] [G loss: -4.736099]\n",
      "[Epoch 0/200] [Batch 110/3166] [D loss: -87.300476] [G loss: -6.170830]\n",
      "[Epoch 0/200] [Batch 115/3166] [D loss: -86.747650] [G loss: -5.598419]\n",
      "[Epoch 0/200] [Batch 120/3166] [D loss: -93.915009] [G loss: -7.193588]\n",
      "[Epoch 0/200] [Batch 125/3166] [D loss: -89.158905] [G loss: -7.406041]\n",
      "[Epoch 0/200] [Batch 130/3166] [D loss: -86.718277] [G loss: -7.908977]\n",
      "[Epoch 0/200] [Batch 135/3166] [D loss: -91.744064] [G loss: -7.183792]\n",
      "[Epoch 0/200] [Batch 140/3166] [D loss: -84.944496] [G loss: -7.121191]\n",
      "[Epoch 0/200] [Batch 145/3166] [D loss: -86.432541] [G loss: -10.487461]\n",
      "[Epoch 0/200] [Batch 150/3166] [D loss: -87.233246] [G loss: -9.749723]\n",
      "[Epoch 0/200] [Batch 155/3166] [D loss: -87.436691] [G loss: -9.677657]\n",
      "[Epoch 0/200] [Batch 160/3166] [D loss: -81.097031] [G loss: -9.179714]\n",
      "[Epoch 0/200] [Batch 165/3166] [D loss: -85.377121] [G loss: -10.813554]\n",
      "[Epoch 0/200] [Batch 170/3166] [D loss: -85.674393] [G loss: -11.062511]\n",
      "[Epoch 0/200] [Batch 175/3166] [D loss: -84.086136] [G loss: -10.988688]\n",
      "[Epoch 0/200] [Batch 180/3166] [D loss: -89.485809] [G loss: -15.316677]\n",
      "[Epoch 0/200] [Batch 185/3166] [D loss: -80.085213] [G loss: -11.960674]\n",
      "[Epoch 0/200] [Batch 190/3166] [D loss: -80.960037] [G loss: -14.212925]\n",
      "[Epoch 0/200] [Batch 195/3166] [D loss: -76.513512] [G loss: -15.167178]\n",
      "[Epoch 0/200] [Batch 200/3166] [D loss: -81.763123] [G loss: -16.806183]\n",
      "[Epoch 0/200] [Batch 205/3166] [D loss: -81.113960] [G loss: -19.044601]\n",
      "[Epoch 0/200] [Batch 210/3166] [D loss: -76.426521] [G loss: -16.252359]\n",
      "[Epoch 0/200] [Batch 215/3166] [D loss: -81.520538] [G loss: -15.684973]\n",
      "[Epoch 0/200] [Batch 220/3166] [D loss: -78.243248] [G loss: -17.110312]\n",
      "[Epoch 0/200] [Batch 225/3166] [D loss: -73.706398] [G loss: -21.516081]\n",
      "[Epoch 0/200] [Batch 230/3166] [D loss: -68.421906] [G loss: -18.663319]\n",
      "[Epoch 0/200] [Batch 235/3166] [D loss: -71.913612] [G loss: -20.289125]\n",
      "[Epoch 0/200] [Batch 240/3166] [D loss: -73.078583] [G loss: -18.724152]\n",
      "[Epoch 0/200] [Batch 245/3166] [D loss: -69.766655] [G loss: -25.151545]\n",
      "[Epoch 0/200] [Batch 250/3166] [D loss: -66.980400] [G loss: -18.001797]\n",
      "[Epoch 0/200] [Batch 255/3166] [D loss: -64.187027] [G loss: -23.564981]\n",
      "[Epoch 0/200] [Batch 260/3166] [D loss: -64.946426] [G loss: -15.229713]\n",
      "[Epoch 0/200] [Batch 265/3166] [D loss: -59.863625] [G loss: -17.994116]\n",
      "[Epoch 0/200] [Batch 270/3166] [D loss: -65.195320] [G loss: -12.228992]\n",
      "[Epoch 0/200] [Batch 275/3166] [D loss: -57.915565] [G loss: -15.873675]\n",
      "[Epoch 0/200] [Batch 280/3166] [D loss: -60.897934] [G loss: -19.046379]\n",
      "[Epoch 0/200] [Batch 285/3166] [D loss: -61.712051] [G loss: -12.143830]\n",
      "[Epoch 0/200] [Batch 290/3166] [D loss: -66.248215] [G loss: -10.310081]\n",
      "[Epoch 0/200] [Batch 295/3166] [D loss: -53.482510] [G loss: -12.177609]\n",
      "[Epoch 0/200] [Batch 300/3166] [D loss: -54.962822] [G loss: -18.796827]\n",
      "[Epoch 0/200] [Batch 305/3166] [D loss: -62.441528] [G loss: -8.592009]\n",
      "[Epoch 0/200] [Batch 310/3166] [D loss: -53.436790] [G loss: -16.184694]\n",
      "[Epoch 0/200] [Batch 315/3166] [D loss: -47.869816] [G loss: -13.458042]\n",
      "[Epoch 0/200] [Batch 320/3166] [D loss: -55.441925] [G loss: -14.361191]\n",
      "[Epoch 0/200] [Batch 325/3166] [D loss: -56.685116] [G loss: -9.200543]\n",
      "[Epoch 0/200] [Batch 330/3166] [D loss: -55.171425] [G loss: -12.051770]\n",
      "[Epoch 0/200] [Batch 335/3166] [D loss: -57.056072] [G loss: -7.359609]\n",
      "[Epoch 0/200] [Batch 340/3166] [D loss: -58.804401] [G loss: -1.272261]\n",
      "[Epoch 0/200] [Batch 345/3166] [D loss: -50.338402] [G loss: -7.017234]\n",
      "[Epoch 0/200] [Batch 350/3166] [D loss: -57.194313] [G loss: -7.258054]\n",
      "[Epoch 0/200] [Batch 355/3166] [D loss: -49.760025] [G loss: -1.087684]\n",
      "[Epoch 0/200] [Batch 360/3166] [D loss: -48.606247] [G loss: -7.041881]\n",
      "[Epoch 0/200] [Batch 365/3166] [D loss: -59.329830] [G loss: -2.671511]\n",
      "[Epoch 0/200] [Batch 370/3166] [D loss: -51.061172] [G loss: -6.751021]\n",
      "[Epoch 0/200] [Batch 375/3166] [D loss: -54.941109] [G loss: -12.688566]\n",
      "[Epoch 0/200] [Batch 380/3166] [D loss: -47.756439] [G loss: -14.610092]\n",
      "[Epoch 0/200] [Batch 385/3166] [D loss: -53.632816] [G loss: -5.464311]\n",
      "[Epoch 0/200] [Batch 390/3166] [D loss: -50.900791] [G loss: -10.530396]\n",
      "[Epoch 0/200] [Batch 395/3166] [D loss: -56.281536] [G loss: -2.754119]\n",
      "[Epoch 0/200] [Batch 400/3166] [D loss: -57.673214] [G loss: -6.830079]\n",
      "[Epoch 0/200] [Batch 405/3166] [D loss: -50.577091] [G loss: -2.776357]\n",
      "[Epoch 0/200] [Batch 410/3166] [D loss: -45.530556] [G loss: -7.995891]\n",
      "[Epoch 0/200] [Batch 415/3166] [D loss: -52.136311] [G loss: -2.201403]\n",
      "[Epoch 0/200] [Batch 420/3166] [D loss: -60.574158] [G loss: -1.402017]\n",
      "[Epoch 0/200] [Batch 425/3166] [D loss: -52.548347] [G loss: -8.097853]\n",
      "[Epoch 0/200] [Batch 430/3166] [D loss: -49.525612] [G loss: -8.894659]\n",
      "[Epoch 0/200] [Batch 435/3166] [D loss: -57.492928] [G loss: -3.274676]\n",
      "[Epoch 0/200] [Batch 440/3166] [D loss: -53.011055] [G loss: -1.957565]\n",
      "[Epoch 0/200] [Batch 445/3166] [D loss: -51.886635] [G loss: -6.128839]\n",
      "[Epoch 0/200] [Batch 450/3166] [D loss: -52.173134] [G loss: -10.095953]\n",
      "[Epoch 0/200] [Batch 455/3166] [D loss: -51.159592] [G loss: -3.260597]\n",
      "[Epoch 0/200] [Batch 460/3166] [D loss: -45.430084] [G loss: -9.232735]\n",
      "[Epoch 0/200] [Batch 465/3166] [D loss: -49.793583] [G loss: 0.205421]\n",
      "[Epoch 0/200] [Batch 470/3166] [D loss: -50.625053] [G loss: -4.143578]\n",
      "[Epoch 0/200] [Batch 475/3166] [D loss: -52.703194] [G loss: -8.804750]\n",
      "[Epoch 0/200] [Batch 480/3166] [D loss: -45.293571] [G loss: -6.123139]\n",
      "[Epoch 0/200] [Batch 485/3166] [D loss: -50.898579] [G loss: -3.175761]\n",
      "[Epoch 0/200] [Batch 490/3166] [D loss: -50.396320] [G loss: -4.689734]\n",
      "[Epoch 0/200] [Batch 495/3166] [D loss: -47.279667] [G loss: -0.646416]\n",
      "[Epoch 0/200] [Batch 500/3166] [D loss: -46.266281] [G loss: -7.059149]\n",
      "[Epoch 0/200] [Batch 505/3166] [D loss: -40.918037] [G loss: -7.661097]\n",
      "[Epoch 0/200] [Batch 510/3166] [D loss: -50.369492] [G loss: -8.915434]\n",
      "[Epoch 0/200] [Batch 515/3166] [D loss: -53.024597] [G loss: 8.357357]\n",
      "[Epoch 0/200] [Batch 520/3166] [D loss: -47.230965] [G loss: -4.202481]\n",
      "[Epoch 0/200] [Batch 525/3166] [D loss: -45.232292] [G loss: -0.344838]\n",
      "[Epoch 0/200] [Batch 530/3166] [D loss: -47.619186] [G loss: -3.932690]\n",
      "[Epoch 0/200] [Batch 535/3166] [D loss: -47.056351] [G loss: -0.573911]\n",
      "[Epoch 0/200] [Batch 540/3166] [D loss: -48.510345] [G loss: 1.036551]\n",
      "[Epoch 0/200] [Batch 545/3166] [D loss: -45.596909] [G loss: -2.259790]\n",
      "[Epoch 0/200] [Batch 550/3166] [D loss: -44.225967] [G loss: -1.340013]\n",
      "[Epoch 0/200] [Batch 555/3166] [D loss: -37.014904] [G loss: -13.511858]\n",
      "[Epoch 0/200] [Batch 560/3166] [D loss: -44.140335] [G loss: -6.059861]\n",
      "[Epoch 0/200] [Batch 565/3166] [D loss: -49.981190] [G loss: 4.687911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 570/3166] [D loss: -36.371948] [G loss: -5.762958]\n",
      "[Epoch 0/200] [Batch 575/3166] [D loss: -42.340996] [G loss: -11.487638]\n",
      "[Epoch 0/200] [Batch 580/3166] [D loss: -35.311115] [G loss: -4.973709]\n",
      "[Epoch 0/200] [Batch 585/3166] [D loss: -37.795242] [G loss: -0.882629]\n",
      "[Epoch 0/200] [Batch 590/3166] [D loss: -38.565647] [G loss: -3.332385]\n",
      "[Epoch 0/200] [Batch 595/3166] [D loss: -37.719543] [G loss: -5.464477]\n",
      "[Epoch 0/200] [Batch 600/3166] [D loss: -45.377098] [G loss: 6.514403]\n",
      "[Epoch 0/200] [Batch 605/3166] [D loss: -43.323120] [G loss: -7.609234]\n",
      "[Epoch 0/200] [Batch 610/3166] [D loss: -45.244095] [G loss: -1.100263]\n",
      "[Epoch 0/200] [Batch 615/3166] [D loss: -41.419701] [G loss: -3.838773]\n",
      "[Epoch 0/200] [Batch 620/3166] [D loss: -40.031860] [G loss: -1.489069]\n",
      "[Epoch 0/200] [Batch 625/3166] [D loss: -46.141190] [G loss: -3.988760]\n",
      "[Epoch 0/200] [Batch 630/3166] [D loss: -32.431259] [G loss: 1.157565]\n",
      "[Epoch 0/200] [Batch 635/3166] [D loss: -45.424957] [G loss: 4.028649]\n",
      "[Epoch 0/200] [Batch 640/3166] [D loss: -41.174721] [G loss: -0.215709]\n",
      "[Epoch 0/200] [Batch 645/3166] [D loss: -42.736504] [G loss: 2.452723]\n",
      "[Epoch 0/200] [Batch 650/3166] [D loss: -39.151157] [G loss: -2.166849]\n",
      "[Epoch 0/200] [Batch 655/3166] [D loss: -35.875786] [G loss: -4.590232]\n",
      "[Epoch 0/200] [Batch 660/3166] [D loss: -36.789413] [G loss: 1.320298]\n",
      "[Epoch 0/200] [Batch 665/3166] [D loss: -43.605179] [G loss: 4.584027]\n",
      "[Epoch 0/200] [Batch 670/3166] [D loss: -44.536057] [G loss: -11.770088]\n",
      "[Epoch 0/200] [Batch 675/3166] [D loss: -40.892075] [G loss: -0.941347]\n",
      "[Epoch 0/200] [Batch 680/3166] [D loss: -42.260559] [G loss: -6.664276]\n",
      "[Epoch 0/200] [Batch 685/3166] [D loss: -34.570992] [G loss: -5.758118]\n",
      "[Epoch 0/200] [Batch 690/3166] [D loss: -38.757282] [G loss: 2.000839]\n",
      "[Epoch 0/200] [Batch 695/3166] [D loss: -40.505768] [G loss: 7.136062]\n",
      "[Epoch 0/200] [Batch 700/3166] [D loss: -37.361107] [G loss: -19.877176]\n",
      "[Epoch 0/200] [Batch 705/3166] [D loss: -37.497360] [G loss: 11.479452]\n",
      "[Epoch 0/200] [Batch 710/3166] [D loss: -40.099308] [G loss: 7.328004]\n",
      "[Epoch 0/200] [Batch 715/3166] [D loss: -41.601009] [G loss: 2.468916]\n",
      "[Epoch 0/200] [Batch 720/3166] [D loss: -34.776245] [G loss: -0.810843]\n",
      "[Epoch 0/200] [Batch 725/3166] [D loss: -40.476223] [G loss: 4.084335]\n",
      "[Epoch 0/200] [Batch 730/3166] [D loss: -39.839664] [G loss: -0.813207]\n",
      "[Epoch 0/200] [Batch 735/3166] [D loss: -38.852390] [G loss: -0.011458]\n",
      "model save at loss = -0.011458\n",
      "[Epoch 0/200] [Batch 740/3166] [D loss: -36.040752] [G loss: -2.247681]\n",
      "[Epoch 0/200] [Batch 745/3166] [D loss: -40.906540] [G loss: 1.275116]\n",
      "[Epoch 0/200] [Batch 750/3166] [D loss: -37.468456] [G loss: -6.299555]\n",
      "[Epoch 0/200] [Batch 755/3166] [D loss: -39.828896] [G loss: 2.526480]\n",
      "[Epoch 0/200] [Batch 760/3166] [D loss: -37.286209] [G loss: 6.832498]\n",
      "[Epoch 0/200] [Batch 765/3166] [D loss: -33.998695] [G loss: 5.619613]\n",
      "[Epoch 0/200] [Batch 770/3166] [D loss: -36.383606] [G loss: 7.114656]\n",
      "[Epoch 0/200] [Batch 775/3166] [D loss: -32.334126] [G loss: -1.688111]\n",
      "[Epoch 0/200] [Batch 780/3166] [D loss: -35.405106] [G loss: 7.913024]\n",
      "[Epoch 0/200] [Batch 785/3166] [D loss: -40.267548] [G loss: 6.545300]\n",
      "[Epoch 0/200] [Batch 790/3166] [D loss: -35.860580] [G loss: -4.954066]\n",
      "[Epoch 0/200] [Batch 795/3166] [D loss: -30.033958] [G loss: -14.756495]\n",
      "[Epoch 0/200] [Batch 800/3166] [D loss: -36.589474] [G loss: -3.721575]\n",
      "[Epoch 0/200] [Batch 805/3166] [D loss: -33.968941] [G loss: 3.144943]\n",
      "[Epoch 0/200] [Batch 810/3166] [D loss: -36.935043] [G loss: 0.435450]\n",
      "[Epoch 0/200] [Batch 815/3166] [D loss: -38.115490] [G loss: 1.446210]\n",
      "[Epoch 0/200] [Batch 820/3166] [D loss: -39.480995] [G loss: -3.041475]\n",
      "[Epoch 0/200] [Batch 825/3166] [D loss: -39.812565] [G loss: -5.965099]\n",
      "[Epoch 0/200] [Batch 830/3166] [D loss: -35.365139] [G loss: -1.691718]\n",
      "[Epoch 0/200] [Batch 835/3166] [D loss: -33.177460] [G loss: 6.785316]\n",
      "[Epoch 0/200] [Batch 840/3166] [D loss: -33.367546] [G loss: 5.159936]\n",
      "[Epoch 0/200] [Batch 845/3166] [D loss: -32.333626] [G loss: 5.542020]\n",
      "[Epoch 0/200] [Batch 850/3166] [D loss: -30.736948] [G loss: -2.081225]\n",
      "[Epoch 0/200] [Batch 855/3166] [D loss: -38.748291] [G loss: 7.717404]\n",
      "[Epoch 0/200] [Batch 860/3166] [D loss: -37.148930] [G loss: 6.865519]\n",
      "[Epoch 0/200] [Batch 865/3166] [D loss: -31.821554] [G loss: 1.894008]\n",
      "[Epoch 0/200] [Batch 870/3166] [D loss: -32.247208] [G loss: 2.456016]\n",
      "[Epoch 0/200] [Batch 875/3166] [D loss: -38.809959] [G loss: 6.224607]\n",
      "[Epoch 0/200] [Batch 880/3166] [D loss: -40.666325] [G loss: 15.397417]\n",
      "[Epoch 0/200] [Batch 885/3166] [D loss: -36.736328] [G loss: -1.200635]\n",
      "[Epoch 0/200] [Batch 890/3166] [D loss: -33.706032] [G loss: 1.574269]\n",
      "[Epoch 0/200] [Batch 895/3166] [D loss: -33.023216] [G loss: 1.030904]\n",
      "[Epoch 0/200] [Batch 900/3166] [D loss: -37.541332] [G loss: 3.133378]\n",
      "[Epoch 0/200] [Batch 905/3166] [D loss: -37.380169] [G loss: -0.306046]\n",
      "[Epoch 0/200] [Batch 910/3166] [D loss: -42.479713] [G loss: 7.091128]\n",
      "[Epoch 0/200] [Batch 915/3166] [D loss: -32.810806] [G loss: 2.448734]\n",
      "[Epoch 0/200] [Batch 920/3166] [D loss: -33.426956] [G loss: 5.091438]\n",
      "[Epoch 0/200] [Batch 925/3166] [D loss: -32.845638] [G loss: 3.371799]\n",
      "[Epoch 0/200] [Batch 930/3166] [D loss: -32.422466] [G loss: 4.595229]\n",
      "[Epoch 0/200] [Batch 935/3166] [D loss: -37.885979] [G loss: 9.085814]\n",
      "[Epoch 0/200] [Batch 940/3166] [D loss: -33.592968] [G loss: 1.999240]\n",
      "[Epoch 0/200] [Batch 945/3166] [D loss: -27.301018] [G loss: -5.580954]\n",
      "[Epoch 0/200] [Batch 950/3166] [D loss: -34.596497] [G loss: -2.861681]\n",
      "[Epoch 0/200] [Batch 955/3166] [D loss: -33.255234] [G loss: 5.758189]\n",
      "[Epoch 0/200] [Batch 960/3166] [D loss: -33.778328] [G loss: 3.151396]\n",
      "[Epoch 0/200] [Batch 965/3166] [D loss: -39.810009] [G loss: -2.967781]\n",
      "[Epoch 0/200] [Batch 970/3166] [D loss: -33.616585] [G loss: 2.264868]\n",
      "[Epoch 0/200] [Batch 975/3166] [D loss: -35.842468] [G loss: -3.644049]\n",
      "[Epoch 0/200] [Batch 980/3166] [D loss: -36.518295] [G loss: -1.822164]\n",
      "[Epoch 0/200] [Batch 985/3166] [D loss: -35.961044] [G loss: 7.822943]\n",
      "[Epoch 0/200] [Batch 990/3166] [D loss: -30.056541] [G loss: -1.868007]\n",
      "[Epoch 0/200] [Batch 995/3166] [D loss: -34.745064] [G loss: 3.093988]\n",
      "[Epoch 0/200] [Batch 1000/3166] [D loss: -37.041878] [G loss: 6.551353]\n",
      "[Epoch 0/200] [Batch 1005/3166] [D loss: -33.671112] [G loss: -4.106403]\n",
      "[Epoch 0/200] [Batch 1010/3166] [D loss: -40.225204] [G loss: 10.578081]\n",
      "[Epoch 0/200] [Batch 1015/3166] [D loss: -34.994492] [G loss: 1.362370]\n",
      "[Epoch 0/200] [Batch 1020/3166] [D loss: -32.284264] [G loss: -0.797937]\n",
      "[Epoch 0/200] [Batch 1025/3166] [D loss: -28.392090] [G loss: 1.369976]\n",
      "[Epoch 0/200] [Batch 1030/3166] [D loss: -30.940773] [G loss: -2.639579]\n",
      "[Epoch 0/200] [Batch 1035/3166] [D loss: -30.337893] [G loss: -2.182786]\n",
      "[Epoch 0/200] [Batch 1040/3166] [D loss: -34.717083] [G loss: 12.093344]\n",
      "[Epoch 0/200] [Batch 1045/3166] [D loss: -33.019821] [G loss: 4.307731]\n",
      "[Epoch 0/200] [Batch 1050/3166] [D loss: -33.797249] [G loss: 5.531611]\n",
      "[Epoch 0/200] [Batch 1055/3166] [D loss: -35.746315] [G loss: 8.428785]\n",
      "[Epoch 0/200] [Batch 1060/3166] [D loss: -35.711441] [G loss: 0.659463]\n",
      "[Epoch 0/200] [Batch 1065/3166] [D loss: -31.210310] [G loss: -1.919061]\n",
      "[Epoch 0/200] [Batch 1070/3166] [D loss: -31.830791] [G loss: -7.562277]\n",
      "[Epoch 0/200] [Batch 1075/3166] [D loss: -30.145515] [G loss: 5.443771]\n",
      "[Epoch 0/200] [Batch 1080/3166] [D loss: -31.860062] [G loss: 5.292239]\n",
      "[Epoch 0/200] [Batch 1085/3166] [D loss: -29.440630] [G loss: -3.945494]\n",
      "[Epoch 0/200] [Batch 1090/3166] [D loss: -28.655346] [G loss: -0.295039]\n",
      "[Epoch 0/200] [Batch 1095/3166] [D loss: -29.817240] [G loss: 4.994271]\n",
      "[Epoch 0/200] [Batch 1100/3166] [D loss: -30.945721] [G loss: 4.123285]\n",
      "[Epoch 0/200] [Batch 1105/3166] [D loss: -30.489702] [G loss: 4.769146]\n",
      "[Epoch 0/200] [Batch 1110/3166] [D loss: -34.539749] [G loss: 3.037018]\n",
      "[Epoch 0/200] [Batch 1115/3166] [D loss: -32.887230] [G loss: 3.332094]\n",
      "[Epoch 0/200] [Batch 1120/3166] [D loss: -37.030586] [G loss: 4.144383]\n",
      "[Epoch 0/200] [Batch 1125/3166] [D loss: -31.602310] [G loss: 6.469305]\n",
      "[Epoch 0/200] [Batch 1130/3166] [D loss: -31.544159] [G loss: -0.338086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 1135/3166] [D loss: -34.290077] [G loss: 5.459374]\n",
      "[Epoch 0/200] [Batch 1140/3166] [D loss: -29.536728] [G loss: 0.569596]\n",
      "[Epoch 0/200] [Batch 1145/3166] [D loss: -29.400902] [G loss: -0.755195]\n",
      "[Epoch 0/200] [Batch 1150/3166] [D loss: -31.591043] [G loss: 12.129928]\n",
      "[Epoch 0/200] [Batch 1155/3166] [D loss: -32.190361] [G loss: 9.707707]\n",
      "[Epoch 0/200] [Batch 1160/3166] [D loss: -29.899092] [G loss: -3.031967]\n",
      "[Epoch 0/200] [Batch 1165/3166] [D loss: -30.087132] [G loss: 1.472071]\n",
      "[Epoch 0/200] [Batch 1170/3166] [D loss: -30.199638] [G loss: -0.379412]\n",
      "[Epoch 0/200] [Batch 1175/3166] [D loss: -30.595467] [G loss: 3.197158]\n",
      "[Epoch 0/200] [Batch 1180/3166] [D loss: -27.538946] [G loss: -4.213981]\n",
      "[Epoch 0/200] [Batch 1185/3166] [D loss: -30.657303] [G loss: 0.577175]\n",
      "[Epoch 0/200] [Batch 1190/3166] [D loss: -33.293884] [G loss: 4.759147]\n",
      "[Epoch 0/200] [Batch 1195/3166] [D loss: -33.838108] [G loss: -1.891172]\n",
      "[Epoch 0/200] [Batch 1200/3166] [D loss: -29.614784] [G loss: -6.010376]\n",
      "[Epoch 0/200] [Batch 1205/3166] [D loss: -35.806511] [G loss: 5.714709]\n",
      "[Epoch 0/200] [Batch 1210/3166] [D loss: -33.092354] [G loss: 3.981738]\n",
      "[Epoch 0/200] [Batch 1215/3166] [D loss: -33.545670] [G loss: -5.528855]\n",
      "[Epoch 0/200] [Batch 1220/3166] [D loss: -27.153980] [G loss: 1.365335]\n",
      "[Epoch 0/200] [Batch 1225/3166] [D loss: -24.765104] [G loss: 3.109613]\n",
      "[Epoch 0/200] [Batch 1230/3166] [D loss: -26.859692] [G loss: 2.404616]\n",
      "[Epoch 0/200] [Batch 1235/3166] [D loss: -30.788462] [G loss: 7.306088]\n",
      "[Epoch 0/200] [Batch 1240/3166] [D loss: -28.960791] [G loss: 3.240824]\n",
      "[Epoch 0/200] [Batch 1245/3166] [D loss: -32.990906] [G loss: 9.069887]\n",
      "[Epoch 0/200] [Batch 1250/3166] [D loss: -33.784977] [G loss: 7.617692]\n",
      "[Epoch 0/200] [Batch 1255/3166] [D loss: -30.756516] [G loss: 1.824526]\n",
      "[Epoch 0/200] [Batch 1260/3166] [D loss: -35.018021] [G loss: 4.216715]\n",
      "[Epoch 0/200] [Batch 1265/3166] [D loss: -28.573788] [G loss: -0.987879]\n",
      "[Epoch 0/200] [Batch 1270/3166] [D loss: -25.670273] [G loss: 6.187334]\n",
      "[Epoch 0/200] [Batch 1275/3166] [D loss: -28.279438] [G loss: 2.504020]\n",
      "[Epoch 0/200] [Batch 1280/3166] [D loss: -28.156797] [G loss: 2.435615]\n",
      "[Epoch 0/200] [Batch 1285/3166] [D loss: -32.650410] [G loss: 11.382233]\n",
      "[Epoch 0/200] [Batch 1290/3166] [D loss: -28.035158] [G loss: 5.325586]\n",
      "[Epoch 0/200] [Batch 1295/3166] [D loss: -29.122810] [G loss: 0.482344]\n",
      "[Epoch 0/200] [Batch 1300/3166] [D loss: -32.806679] [G loss: 7.715219]\n",
      "[Epoch 0/200] [Batch 1305/3166] [D loss: -22.854868] [G loss: -0.039998]\n",
      "[Epoch 0/200] [Batch 1310/3166] [D loss: -30.944975] [G loss: 3.949475]\n",
      "[Epoch 0/200] [Batch 1315/3166] [D loss: -31.417564] [G loss: -6.204651]\n",
      "[Epoch 0/200] [Batch 1320/3166] [D loss: -25.889479] [G loss: 7.475502]\n",
      "[Epoch 0/200] [Batch 1325/3166] [D loss: -28.840431] [G loss: -0.597968]\n",
      "[Epoch 0/200] [Batch 1330/3166] [D loss: -30.335661] [G loss: 9.593445]\n",
      "[Epoch 0/200] [Batch 1335/3166] [D loss: -26.048674] [G loss: 6.293388]\n",
      "[Epoch 0/200] [Batch 1340/3166] [D loss: -26.041927] [G loss: 6.231809]\n",
      "[Epoch 0/200] [Batch 1345/3166] [D loss: -27.119526] [G loss: -10.183897]\n",
      "[Epoch 0/200] [Batch 1350/3166] [D loss: -28.867376] [G loss: 2.747600]\n",
      "[Epoch 0/200] [Batch 1355/3166] [D loss: -33.540886] [G loss: 2.459380]\n",
      "[Epoch 0/200] [Batch 1360/3166] [D loss: -29.826843] [G loss: 6.707412]\n",
      "[Epoch 0/200] [Batch 1365/3166] [D loss: -27.377142] [G loss: 8.479994]\n",
      "[Epoch 0/200] [Batch 1370/3166] [D loss: -25.964247] [G loss: 6.105051]\n",
      "[Epoch 0/200] [Batch 1375/3166] [D loss: -30.164194] [G loss: 8.129643]\n",
      "[Epoch 0/200] [Batch 1380/3166] [D loss: -29.844780] [G loss: 1.013232]\n",
      "[Epoch 0/200] [Batch 1385/3166] [D loss: -30.113062] [G loss: 3.737141]\n",
      "[Epoch 0/200] [Batch 1390/3166] [D loss: -24.395193] [G loss: 6.052729]\n",
      "[Epoch 0/200] [Batch 1395/3166] [D loss: -31.450613] [G loss: 1.569477]\n",
      "[Epoch 0/200] [Batch 1400/3166] [D loss: -26.284504] [G loss: -7.054866]\n",
      "[Epoch 0/200] [Batch 1405/3166] [D loss: -27.619486] [G loss: -0.413801]\n",
      "[Epoch 0/200] [Batch 1410/3166] [D loss: -25.036045] [G loss: -1.580423]\n",
      "[Epoch 0/200] [Batch 1415/3166] [D loss: -26.671869] [G loss: 5.172171]\n",
      "[Epoch 0/200] [Batch 1420/3166] [D loss: -30.976036] [G loss: 0.076330]\n",
      "[Epoch 0/200] [Batch 1425/3166] [D loss: -29.207991] [G loss: 2.423082]\n",
      "[Epoch 0/200] [Batch 1430/3166] [D loss: -28.052998] [G loss: 12.321975]\n",
      "[Epoch 0/200] [Batch 1435/3166] [D loss: -26.186127] [G loss: 5.945254]\n",
      "[Epoch 0/200] [Batch 1440/3166] [D loss: -28.312147] [G loss: 1.875006]\n",
      "[Epoch 0/200] [Batch 1445/3166] [D loss: -27.795120] [G loss: 7.925671]\n",
      "[Epoch 0/200] [Batch 1450/3166] [D loss: -27.464853] [G loss: 4.964390]\n",
      "[Epoch 0/200] [Batch 1455/3166] [D loss: -24.899261] [G loss: 1.214913]\n",
      "[Epoch 0/200] [Batch 1460/3166] [D loss: -27.166475] [G loss: -2.671961]\n",
      "[Epoch 0/200] [Batch 1465/3166] [D loss: -31.463100] [G loss: 7.563987]\n",
      "[Epoch 0/200] [Batch 1470/3166] [D loss: -27.212320] [G loss: 5.753350]\n",
      "[Epoch 0/200] [Batch 1475/3166] [D loss: -25.284550] [G loss: 3.423453]\n",
      "[Epoch 0/200] [Batch 1480/3166] [D loss: -27.918148] [G loss: 3.844318]\n",
      "[Epoch 0/200] [Batch 1485/3166] [D loss: -28.418671] [G loss: 5.228382]\n",
      "[Epoch 0/200] [Batch 1490/3166] [D loss: -30.070559] [G loss: 10.646889]\n",
      "[Epoch 0/200] [Batch 1495/3166] [D loss: -26.170637] [G loss: 2.209832]\n",
      "[Epoch 0/200] [Batch 1500/3166] [D loss: -29.093136] [G loss: 7.216027]\n",
      "[Epoch 0/200] [Batch 1505/3166] [D loss: -26.909462] [G loss: 7.381898]\n",
      "[Epoch 0/200] [Batch 1510/3166] [D loss: -27.102371] [G loss: 3.514150]\n",
      "[Epoch 0/200] [Batch 1515/3166] [D loss: -30.799473] [G loss: 2.864619]\n",
      "[Epoch 0/200] [Batch 1520/3166] [D loss: -27.993893] [G loss: 9.379474]\n",
      "[Epoch 0/200] [Batch 1525/3166] [D loss: -30.902132] [G loss: 1.034669]\n",
      "[Epoch 0/200] [Batch 1530/3166] [D loss: -28.759857] [G loss: 2.905350]\n",
      "[Epoch 0/200] [Batch 1535/3166] [D loss: -28.780350] [G loss: 12.125944]\n",
      "[Epoch 0/200] [Batch 1540/3166] [D loss: -25.323418] [G loss: 5.042586]\n",
      "[Epoch 0/200] [Batch 1545/3166] [D loss: -24.999805] [G loss: 0.974452]\n",
      "[Epoch 0/200] [Batch 1550/3166] [D loss: -28.463528] [G loss: 6.616360]\n",
      "[Epoch 0/200] [Batch 1555/3166] [D loss: -27.500505] [G loss: 7.518604]\n",
      "[Epoch 0/200] [Batch 1560/3166] [D loss: -30.610573] [G loss: 12.993807]\n",
      "[Epoch 0/200] [Batch 1565/3166] [D loss: -27.330791] [G loss: 3.119946]\n",
      "[Epoch 0/200] [Batch 1570/3166] [D loss: -27.464354] [G loss: 3.983906]\n",
      "[Epoch 0/200] [Batch 1575/3166] [D loss: -29.061249] [G loss: 7.782968]\n",
      "[Epoch 0/200] [Batch 1580/3166] [D loss: -28.529411] [G loss: 0.387350]\n",
      "[Epoch 0/200] [Batch 1585/3166] [D loss: -28.669397] [G loss: 6.409958]\n",
      "[Epoch 0/200] [Batch 1590/3166] [D loss: -29.448544] [G loss: 4.204976]\n",
      "[Epoch 0/200] [Batch 1595/3166] [D loss: -26.200180] [G loss: -4.493723]\n",
      "[Epoch 0/200] [Batch 1600/3166] [D loss: -28.476770] [G loss: 4.287057]\n",
      "[Epoch 0/200] [Batch 1605/3166] [D loss: -30.376886] [G loss: 2.333876]\n",
      "[Epoch 0/200] [Batch 1610/3166] [D loss: -29.936798] [G loss: 12.139469]\n",
      "[Epoch 0/200] [Batch 1615/3166] [D loss: -29.217768] [G loss: 10.740517]\n",
      "[Epoch 0/200] [Batch 1620/3166] [D loss: -28.602751] [G loss: 7.036893]\n",
      "[Epoch 0/200] [Batch 1625/3166] [D loss: -30.732656] [G loss: 3.789625]\n",
      "[Epoch 0/200] [Batch 1630/3166] [D loss: -26.773014] [G loss: 3.246942]\n",
      "[Epoch 0/200] [Batch 1635/3166] [D loss: -29.039000] [G loss: 6.732093]\n",
      "[Epoch 0/200] [Batch 1640/3166] [D loss: -30.416298] [G loss: -1.294952]\n",
      "[Epoch 0/200] [Batch 1645/3166] [D loss: -27.451569] [G loss: 7.033992]\n",
      "[Epoch 0/200] [Batch 1650/3166] [D loss: -30.977041] [G loss: 3.074057]\n",
      "[Epoch 0/200] [Batch 1655/3166] [D loss: -26.354229] [G loss: 7.877752]\n",
      "[Epoch 0/200] [Batch 1660/3166] [D loss: -27.055923] [G loss: 1.648934]\n",
      "[Epoch 0/200] [Batch 1665/3166] [D loss: -28.956154] [G loss: -0.459138]\n",
      "[Epoch 0/200] [Batch 1670/3166] [D loss: -28.933651] [G loss: 5.608132]\n",
      "[Epoch 0/200] [Batch 1675/3166] [D loss: -30.447578] [G loss: 5.234065]\n",
      "[Epoch 0/200] [Batch 1680/3166] [D loss: -27.187767] [G loss: 12.251176]\n",
      "[Epoch 0/200] [Batch 1685/3166] [D loss: -28.761845] [G loss: 5.756462]\n",
      "[Epoch 0/200] [Batch 1690/3166] [D loss: -31.349890] [G loss: 6.338621]\n",
      "[Epoch 0/200] [Batch 1695/3166] [D loss: -31.235638] [G loss: 13.149061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 1700/3166] [D loss: -27.491570] [G loss: 5.687381]\n",
      "[Epoch 0/200] [Batch 1705/3166] [D loss: -26.989864] [G loss: 5.662131]\n",
      "[Epoch 0/200] [Batch 1710/3166] [D loss: -29.048790] [G loss: 0.654930]\n",
      "[Epoch 0/200] [Batch 1715/3166] [D loss: -30.098579] [G loss: 6.492284]\n",
      "[Epoch 0/200] [Batch 1720/3166] [D loss: -28.382822] [G loss: -2.826240]\n",
      "[Epoch 0/200] [Batch 1725/3166] [D loss: -33.570965] [G loss: 4.664761]\n",
      "[Epoch 0/200] [Batch 1730/3166] [D loss: -27.397285] [G loss: 9.017362]\n",
      "[Epoch 0/200] [Batch 1735/3166] [D loss: -30.772480] [G loss: 15.295927]\n",
      "[Epoch 0/200] [Batch 1740/3166] [D loss: -32.140381] [G loss: 6.441147]\n",
      "[Epoch 0/200] [Batch 1745/3166] [D loss: -26.533167] [G loss: 7.509861]\n",
      "[Epoch 0/200] [Batch 1750/3166] [D loss: -27.550083] [G loss: 0.284115]\n",
      "[Epoch 0/200] [Batch 1755/3166] [D loss: -30.459370] [G loss: 7.343574]\n",
      "[Epoch 0/200] [Batch 1760/3166] [D loss: -24.185223] [G loss: 11.227110]\n",
      "[Epoch 0/200] [Batch 1765/3166] [D loss: -28.647287] [G loss: 6.509782]\n",
      "[Epoch 0/200] [Batch 1770/3166] [D loss: -32.699894] [G loss: 4.851538]\n",
      "[Epoch 0/200] [Batch 1775/3166] [D loss: -30.965796] [G loss: 10.246407]\n",
      "[Epoch 0/200] [Batch 1780/3166] [D loss: -31.409508] [G loss: 7.223265]\n",
      "[Epoch 0/200] [Batch 1785/3166] [D loss: -28.357199] [G loss: 7.785011]\n",
      "[Epoch 0/200] [Batch 1790/3166] [D loss: -29.328945] [G loss: -0.440865]\n",
      "[Epoch 0/200] [Batch 1795/3166] [D loss: -29.250011] [G loss: 4.321493]\n",
      "[Epoch 0/200] [Batch 1800/3166] [D loss: -26.976360] [G loss: 11.426426]\n",
      "[Epoch 0/200] [Batch 1805/3166] [D loss: -29.746384] [G loss: 8.086283]\n",
      "[Epoch 0/200] [Batch 1810/3166] [D loss: -29.937565] [G loss: 15.741751]\n",
      "[Epoch 0/200] [Batch 1815/3166] [D loss: -29.435991] [G loss: 11.751817]\n",
      "[Epoch 0/200] [Batch 1820/3166] [D loss: -26.841679] [G loss: 1.425080]\n",
      "[Epoch 0/200] [Batch 1825/3166] [D loss: -22.743031] [G loss: 6.265247]\n",
      "[Epoch 0/200] [Batch 1830/3166] [D loss: -29.881992] [G loss: 5.295866]\n",
      "[Epoch 0/200] [Batch 1835/3166] [D loss: -29.439228] [G loss: 6.615288]\n",
      "[Epoch 0/200] [Batch 1840/3166] [D loss: -29.362377] [G loss: -1.055572]\n",
      "[Epoch 0/200] [Batch 1845/3166] [D loss: -27.611572] [G loss: 15.483649]\n",
      "[Epoch 0/200] [Batch 1850/3166] [D loss: -30.895372] [G loss: 10.397505]\n",
      "[Epoch 0/200] [Batch 1855/3166] [D loss: -30.759581] [G loss: 7.415856]\n",
      "[Epoch 0/200] [Batch 1860/3166] [D loss: -31.867199] [G loss: 6.371580]\n",
      "[Epoch 0/200] [Batch 1865/3166] [D loss: -27.424503] [G loss: 5.439202]\n",
      "[Epoch 0/200] [Batch 1870/3166] [D loss: -29.795078] [G loss: 4.357961]\n",
      "[Epoch 0/200] [Batch 1875/3166] [D loss: -29.144083] [G loss: 1.856883]\n",
      "[Epoch 0/200] [Batch 1880/3166] [D loss: -29.200190] [G loss: 6.060953]\n",
      "[Epoch 0/200] [Batch 1885/3166] [D loss: -32.029602] [G loss: -1.255149]\n",
      "[Epoch 0/200] [Batch 1890/3166] [D loss: -28.234814] [G loss: 9.246460]\n",
      "[Epoch 0/200] [Batch 1895/3166] [D loss: -25.788912] [G loss: 8.746782]\n",
      "[Epoch 0/200] [Batch 1900/3166] [D loss: -30.366642] [G loss: -6.215264]\n",
      "[Epoch 0/200] [Batch 1905/3166] [D loss: -30.861755] [G loss: 5.430034]\n",
      "[Epoch 0/200] [Batch 1910/3166] [D loss: -29.925413] [G loss: 10.420584]\n",
      "[Epoch 0/200] [Batch 1915/3166] [D loss: -34.067535] [G loss: 13.216534]\n",
      "[Epoch 0/200] [Batch 1920/3166] [D loss: -28.012108] [G loss: 5.296713]\n",
      "[Epoch 0/200] [Batch 1925/3166] [D loss: -26.359686] [G loss: 6.754424]\n",
      "[Epoch 0/200] [Batch 1930/3166] [D loss: -29.372963] [G loss: 6.012873]\n",
      "[Epoch 0/200] [Batch 1935/3166] [D loss: -29.968178] [G loss: 4.184473]\n",
      "[Epoch 0/200] [Batch 1940/3166] [D loss: -26.354553] [G loss: 9.398346]\n",
      "[Epoch 0/200] [Batch 1945/3166] [D loss: -28.668921] [G loss: 8.684448]\n",
      "[Epoch 0/200] [Batch 1950/3166] [D loss: -28.727970] [G loss: 13.972929]\n",
      "[Epoch 0/200] [Batch 1955/3166] [D loss: -29.697220] [G loss: 5.520866]\n",
      "[Epoch 0/200] [Batch 1960/3166] [D loss: -26.308310] [G loss: -0.109061]\n",
      "[Epoch 0/200] [Batch 1965/3166] [D loss: -28.043940] [G loss: 0.882836]\n",
      "[Epoch 0/200] [Batch 1970/3166] [D loss: -28.245653] [G loss: 1.622363]\n",
      "[Epoch 0/200] [Batch 1975/3166] [D loss: -25.710413] [G loss: 3.359593]\n",
      "[Epoch 0/200] [Batch 1980/3166] [D loss: -28.076296] [G loss: 7.394541]\n",
      "[Epoch 0/200] [Batch 1985/3166] [D loss: -33.363670] [G loss: -0.617196]\n",
      "[Epoch 0/200] [Batch 1990/3166] [D loss: -26.389429] [G loss: 4.340361]\n",
      "[Epoch 0/200] [Batch 1995/3166] [D loss: -26.839937] [G loss: -0.885422]\n",
      "[Epoch 0/200] [Batch 2000/3166] [D loss: -28.972166] [G loss: 4.474196]\n",
      "[Epoch 0/200] [Batch 2005/3166] [D loss: -31.893753] [G loss: 8.462071]\n",
      "[Epoch 0/200] [Batch 2010/3166] [D loss: -28.329027] [G loss: 0.805695]\n",
      "[Epoch 0/200] [Batch 2015/3166] [D loss: -27.791229] [G loss: 7.629646]\n",
      "[Epoch 0/200] [Batch 2020/3166] [D loss: -28.061821] [G loss: 14.567520]\n",
      "[Epoch 0/200] [Batch 2025/3166] [D loss: -28.610067] [G loss: 5.688357]\n",
      "[Epoch 0/200] [Batch 2030/3166] [D loss: -31.163733] [G loss: 8.606713]\n",
      "[Epoch 0/200] [Batch 2035/3166] [D loss: -28.228014] [G loss: 7.467938]\n",
      "[Epoch 0/200] [Batch 2040/3166] [D loss: -27.684669] [G loss: 8.535275]\n",
      "[Epoch 0/200] [Batch 2045/3166] [D loss: -26.873882] [G loss: 1.219130]\n",
      "[Epoch 0/200] [Batch 2050/3166] [D loss: -31.735434] [G loss: 9.974069]\n",
      "[Epoch 0/200] [Batch 2055/3166] [D loss: -30.932066] [G loss: 3.208565]\n",
      "[Epoch 0/200] [Batch 2060/3166] [D loss: -29.045900] [G loss: 7.030585]\n",
      "[Epoch 0/200] [Batch 2065/3166] [D loss: -28.108658] [G loss: 4.835582]\n",
      "[Epoch 0/200] [Batch 2070/3166] [D loss: -29.617163] [G loss: 1.993458]\n",
      "[Epoch 0/200] [Batch 2075/3166] [D loss: -31.019936] [G loss: 4.989197]\n",
      "[Epoch 0/200] [Batch 2080/3166] [D loss: -30.601803] [G loss: 0.627365]\n",
      "[Epoch 0/200] [Batch 2085/3166] [D loss: -27.149654] [G loss: 7.752358]\n",
      "[Epoch 0/200] [Batch 2090/3166] [D loss: -26.306593] [G loss: 10.194406]\n",
      "[Epoch 0/200] [Batch 2095/3166] [D loss: -25.955669] [G loss: 5.745212]\n",
      "[Epoch 0/200] [Batch 2100/3166] [D loss: -29.829533] [G loss: 0.171605]\n",
      "[Epoch 0/200] [Batch 2105/3166] [D loss: -28.299919] [G loss: -0.666326]\n",
      "[Epoch 0/200] [Batch 2110/3166] [D loss: -27.807158] [G loss: 12.018953]\n",
      "[Epoch 0/200] [Batch 2115/3166] [D loss: -32.090420] [G loss: 13.477110]\n",
      "[Epoch 0/200] [Batch 2120/3166] [D loss: -28.052626] [G loss: 5.116080]\n",
      "[Epoch 0/200] [Batch 2125/3166] [D loss: -26.977535] [G loss: 9.043495]\n",
      "[Epoch 0/200] [Batch 2130/3166] [D loss: -30.738518] [G loss: 0.349015]\n",
      "[Epoch 0/200] [Batch 2135/3166] [D loss: -27.622641] [G loss: 9.026749]\n",
      "[Epoch 0/200] [Batch 2140/3166] [D loss: -28.645184] [G loss: 2.982817]\n",
      "[Epoch 0/200] [Batch 2145/3166] [D loss: -28.620762] [G loss: 4.774010]\n",
      "[Epoch 0/200] [Batch 2150/3166] [D loss: -29.416679] [G loss: 11.686093]\n",
      "[Epoch 0/200] [Batch 2155/3166] [D loss: -25.333843] [G loss: 0.916841]\n",
      "[Epoch 0/200] [Batch 2160/3166] [D loss: -27.703884] [G loss: 7.474060]\n",
      "[Epoch 0/200] [Batch 2165/3166] [D loss: -31.115795] [G loss: 15.975295]\n",
      "[Epoch 0/200] [Batch 2170/3166] [D loss: -31.075663] [G loss: 17.625225]\n",
      "[Epoch 0/200] [Batch 2175/3166] [D loss: -28.422159] [G loss: 3.516370]\n",
      "[Epoch 0/200] [Batch 2180/3166] [D loss: -29.953953] [G loss: 10.158329]\n",
      "[Epoch 0/200] [Batch 2185/3166] [D loss: -25.788239] [G loss: 0.230460]\n",
      "[Epoch 0/200] [Batch 2190/3166] [D loss: -28.515747] [G loss: 13.969536]\n",
      "[Epoch 0/200] [Batch 2195/3166] [D loss: -29.536507] [G loss: 5.756731]\n",
      "[Epoch 0/200] [Batch 2200/3166] [D loss: -26.416962] [G loss: -1.119717]\n",
      "[Epoch 0/200] [Batch 2205/3166] [D loss: -28.092266] [G loss: -2.273190]\n",
      "[Epoch 0/200] [Batch 2210/3166] [D loss: -24.642635] [G loss: -3.711685]\n",
      "[Epoch 0/200] [Batch 2215/3166] [D loss: -28.195644] [G loss: 1.324132]\n",
      "[Epoch 0/200] [Batch 2220/3166] [D loss: -25.245293] [G loss: 8.624802]\n",
      "[Epoch 0/200] [Batch 2225/3166] [D loss: -28.876343] [G loss: 15.072576]\n",
      "[Epoch 0/200] [Batch 2230/3166] [D loss: -28.706364] [G loss: 9.020247]\n",
      "[Epoch 0/200] [Batch 2235/3166] [D loss: -24.431047] [G loss: 2.858646]\n",
      "[Epoch 0/200] [Batch 2240/3166] [D loss: -29.137623] [G loss: 7.244189]\n",
      "[Epoch 0/200] [Batch 2245/3166] [D loss: -29.379545] [G loss: 6.025634]\n",
      "[Epoch 0/200] [Batch 2250/3166] [D loss: -31.877487] [G loss: 8.228397]\n",
      "[Epoch 0/200] [Batch 2255/3166] [D loss: -27.646748] [G loss: 0.666505]\n",
      "[Epoch 0/200] [Batch 2260/3166] [D loss: -27.897514] [G loss: 13.088033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 2265/3166] [D loss: -29.634262] [G loss: 5.983769]\n",
      "[Epoch 0/200] [Batch 2270/3166] [D loss: -26.926226] [G loss: 5.779732]\n",
      "[Epoch 0/200] [Batch 2275/3166] [D loss: -27.621412] [G loss: -1.880738]\n",
      "[Epoch 0/200] [Batch 2280/3166] [D loss: -28.666771] [G loss: 8.629138]\n",
      "[Epoch 0/200] [Batch 2285/3166] [D loss: -29.955847] [G loss: 13.313465]\n",
      "[Epoch 0/200] [Batch 2290/3166] [D loss: -25.677214] [G loss: 7.328914]\n",
      "[Epoch 0/200] [Batch 2295/3166] [D loss: -27.444889] [G loss: 9.664263]\n",
      "[Epoch 0/200] [Batch 2300/3166] [D loss: -24.415693] [G loss: -0.272440]\n",
      "[Epoch 0/200] [Batch 2305/3166] [D loss: -29.637550] [G loss: 3.931837]\n",
      "[Epoch 0/200] [Batch 2310/3166] [D loss: -28.816452] [G loss: 11.727690]\n",
      "[Epoch 0/200] [Batch 2315/3166] [D loss: -29.394266] [G loss: 2.092237]\n",
      "[Epoch 0/200] [Batch 2320/3166] [D loss: -28.275894] [G loss: 4.304096]\n",
      "[Epoch 0/200] [Batch 2325/3166] [D loss: -26.113033] [G loss: 10.657660]\n",
      "[Epoch 0/200] [Batch 2330/3166] [D loss: -29.105040] [G loss: 4.669844]\n",
      "[Epoch 0/200] [Batch 2335/3166] [D loss: -24.600052] [G loss: 2.357134]\n",
      "[Epoch 0/200] [Batch 2340/3166] [D loss: -31.107784] [G loss: 1.873978]\n",
      "[Epoch 0/200] [Batch 2345/3166] [D loss: -30.610453] [G loss: 4.338345]\n",
      "[Epoch 0/200] [Batch 2350/3166] [D loss: -29.973278] [G loss: 13.365856]\n",
      "[Epoch 0/200] [Batch 2355/3166] [D loss: -30.388823] [G loss: 7.296223]\n",
      "[Epoch 0/200] [Batch 2360/3166] [D loss: -27.431990] [G loss: 4.176495]\n",
      "[Epoch 0/200] [Batch 2365/3166] [D loss: -29.698200] [G loss: 0.614677]\n",
      "[Epoch 0/200] [Batch 2370/3166] [D loss: -29.179482] [G loss: 1.442184]\n",
      "[Epoch 0/200] [Batch 2375/3166] [D loss: -23.384277] [G loss: 5.966412]\n",
      "[Epoch 0/200] [Batch 2380/3166] [D loss: -29.601059] [G loss: 4.197771]\n",
      "[Epoch 0/200] [Batch 2385/3166] [D loss: -27.937401] [G loss: 5.666897]\n",
      "[Epoch 0/200] [Batch 2390/3166] [D loss: -30.932261] [G loss: 3.538355]\n",
      "[Epoch 0/200] [Batch 2395/3166] [D loss: -23.760649] [G loss: 1.235271]\n",
      "[Epoch 0/200] [Batch 2400/3166] [D loss: -28.389175] [G loss: 3.880677]\n",
      "[Epoch 0/200] [Batch 2405/3166] [D loss: -27.578678] [G loss: 6.486232]\n",
      "[Epoch 0/200] [Batch 2410/3166] [D loss: -26.304146] [G loss: 12.064113]\n",
      "[Epoch 0/200] [Batch 2415/3166] [D loss: -28.636189] [G loss: 5.022443]\n",
      "[Epoch 0/200] [Batch 2420/3166] [D loss: -31.019093] [G loss: -0.596472]\n",
      "[Epoch 0/200] [Batch 2425/3166] [D loss: -27.198298] [G loss: 6.157932]\n",
      "[Epoch 0/200] [Batch 2430/3166] [D loss: -26.561989] [G loss: 6.493238]\n",
      "[Epoch 0/200] [Batch 2435/3166] [D loss: -26.814741] [G loss: 3.915934]\n",
      "[Epoch 0/200] [Batch 2440/3166] [D loss: -31.411659] [G loss: 6.467627]\n",
      "[Epoch 0/200] [Batch 2445/3166] [D loss: -28.304657] [G loss: 5.431273]\n",
      "[Epoch 0/200] [Batch 2450/3166] [D loss: -29.655491] [G loss: 6.740263]\n",
      "[Epoch 0/200] [Batch 2455/3166] [D loss: -26.988596] [G loss: 6.413350]\n",
      "[Epoch 0/200] [Batch 2460/3166] [D loss: -28.949635] [G loss: 2.809050]\n",
      "[Epoch 0/200] [Batch 2465/3166] [D loss: -26.574226] [G loss: 5.681916]\n",
      "[Epoch 0/200] [Batch 2470/3166] [D loss: -29.495838] [G loss: 11.557026]\n",
      "[Epoch 0/200] [Batch 2475/3166] [D loss: -24.142477] [G loss: 2.862844]\n",
      "[Epoch 0/200] [Batch 2480/3166] [D loss: -26.145643] [G loss: 1.565346]\n",
      "[Epoch 0/200] [Batch 2485/3166] [D loss: -30.854240] [G loss: 9.886769]\n",
      "[Epoch 0/200] [Batch 2490/3166] [D loss: -24.658735] [G loss: 1.473858]\n",
      "[Epoch 0/200] [Batch 2495/3166] [D loss: -27.833084] [G loss: 7.838320]\n",
      "[Epoch 0/200] [Batch 2500/3166] [D loss: -27.671179] [G loss: 5.628818]\n",
      "[Epoch 0/200] [Batch 2505/3166] [D loss: -29.469633] [G loss: 10.046659]\n",
      "[Epoch 0/200] [Batch 2510/3166] [D loss: -28.708414] [G loss: 6.026593]\n",
      "[Epoch 0/200] [Batch 2515/3166] [D loss: -27.213888] [G loss: -0.262393]\n",
      "[Epoch 0/200] [Batch 2520/3166] [D loss: -26.996689] [G loss: 5.210745]\n",
      "[Epoch 0/200] [Batch 2525/3166] [D loss: -28.033207] [G loss: 5.956326]\n",
      "[Epoch 0/200] [Batch 2530/3166] [D loss: -28.388557] [G loss: 10.700214]\n",
      "[Epoch 0/200] [Batch 2535/3166] [D loss: -26.272501] [G loss: 6.320034]\n",
      "[Epoch 0/200] [Batch 2540/3166] [D loss: -29.737272] [G loss: 6.742923]\n",
      "[Epoch 0/200] [Batch 2545/3166] [D loss: -25.040878] [G loss: -1.381536]\n",
      "[Epoch 0/200] [Batch 2550/3166] [D loss: -27.718306] [G loss: 3.285518]\n",
      "[Epoch 0/200] [Batch 2555/3166] [D loss: -27.065487] [G loss: 6.199975]\n",
      "[Epoch 0/200] [Batch 2560/3166] [D loss: -27.738628] [G loss: -0.042791]\n",
      "[Epoch 0/200] [Batch 2565/3166] [D loss: -29.665518] [G loss: 8.623482]\n",
      "[Epoch 0/200] [Batch 2570/3166] [D loss: -30.774237] [G loss: 3.941371]\n",
      "[Epoch 0/200] [Batch 2575/3166] [D loss: -27.711876] [G loss: 1.538031]\n",
      "[Epoch 0/200] [Batch 2580/3166] [D loss: -29.241768] [G loss: 16.728241]\n",
      "[Epoch 0/200] [Batch 2585/3166] [D loss: -28.563932] [G loss: 1.676066]\n",
      "[Epoch 0/200] [Batch 2590/3166] [D loss: -28.852827] [G loss: -1.921964]\n",
      "[Epoch 0/200] [Batch 2595/3166] [D loss: -24.362705] [G loss: 4.977281]\n",
      "[Epoch 0/200] [Batch 2600/3166] [D loss: -27.658283] [G loss: 11.096413]\n",
      "[Epoch 0/200] [Batch 2605/3166] [D loss: -26.312931] [G loss: 8.189812]\n",
      "[Epoch 0/200] [Batch 2610/3166] [D loss: -29.197121] [G loss: 5.667052]\n",
      "[Epoch 0/200] [Batch 2615/3166] [D loss: -31.053444] [G loss: 9.358950]\n",
      "[Epoch 0/200] [Batch 2620/3166] [D loss: -28.705154] [G loss: 15.890638]\n",
      "[Epoch 0/200] [Batch 2625/3166] [D loss: -26.776943] [G loss: 5.080422]\n",
      "[Epoch 0/200] [Batch 2630/3166] [D loss: -28.022430] [G loss: 9.698006]\n",
      "[Epoch 0/200] [Batch 2635/3166] [D loss: -26.570930] [G loss: 8.686409]\n",
      "[Epoch 0/200] [Batch 2640/3166] [D loss: -29.224316] [G loss: 11.240504]\n",
      "[Epoch 0/200] [Batch 2645/3166] [D loss: -27.562099] [G loss: -0.147925]\n",
      "[Epoch 0/200] [Batch 2650/3166] [D loss: -26.813314] [G loss: -1.697400]\n",
      "[Epoch 0/200] [Batch 2655/3166] [D loss: -30.032850] [G loss: 6.560470]\n",
      "[Epoch 0/200] [Batch 2660/3166] [D loss: -26.770067] [G loss: 12.971319]\n",
      "[Epoch 0/200] [Batch 2665/3166] [D loss: -26.537617] [G loss: 0.109168]\n",
      "[Epoch 0/200] [Batch 2670/3166] [D loss: -29.273315] [G loss: 7.629585]\n",
      "[Epoch 0/200] [Batch 2675/3166] [D loss: -27.422867] [G loss: 6.260741]\n",
      "[Epoch 0/200] [Batch 2680/3166] [D loss: -27.500084] [G loss: 0.151686]\n",
      "[Epoch 0/200] [Batch 2685/3166] [D loss: -30.399731] [G loss: 1.614447]\n",
      "[Epoch 0/200] [Batch 2690/3166] [D loss: -25.827976] [G loss: 3.646051]\n",
      "[Epoch 0/200] [Batch 2695/3166] [D loss: -28.903101] [G loss: 8.222183]\n",
      "[Epoch 0/200] [Batch 2700/3166] [D loss: -25.006586] [G loss: 8.605145]\n",
      "[Epoch 0/200] [Batch 2705/3166] [D loss: -27.914547] [G loss: 9.738546]\n",
      "[Epoch 0/200] [Batch 2710/3166] [D loss: -25.728874] [G loss: 7.617896]\n",
      "[Epoch 0/200] [Batch 2715/3166] [D loss: -27.169247] [G loss: 10.276460]\n",
      "[Epoch 0/200] [Batch 2720/3166] [D loss: -28.137259] [G loss: 3.612682]\n",
      "[Epoch 0/200] [Batch 2725/3166] [D loss: -27.465294] [G loss: 7.157557]\n",
      "[Epoch 0/200] [Batch 2730/3166] [D loss: -25.987598] [G loss: 6.395552]\n",
      "[Epoch 0/200] [Batch 2735/3166] [D loss: -29.953171] [G loss: 3.859191]\n",
      "[Epoch 0/200] [Batch 2740/3166] [D loss: -25.733252] [G loss: 4.546877]\n",
      "[Epoch 0/200] [Batch 2745/3166] [D loss: -29.659351] [G loss: 0.921272]\n",
      "[Epoch 0/200] [Batch 2750/3166] [D loss: -33.120556] [G loss: 7.369251]\n",
      "[Epoch 0/200] [Batch 2755/3166] [D loss: -26.994797] [G loss: 5.406623]\n",
      "[Epoch 0/200] [Batch 2760/3166] [D loss: -27.042202] [G loss: 9.834296]\n",
      "[Epoch 0/200] [Batch 2765/3166] [D loss: -26.482523] [G loss: 8.023418]\n",
      "[Epoch 0/200] [Batch 2770/3166] [D loss: -28.768576] [G loss: 0.485961]\n",
      "[Epoch 0/200] [Batch 2775/3166] [D loss: -23.797138] [G loss: 4.708103]\n",
      "[Epoch 0/200] [Batch 2780/3166] [D loss: -27.060234] [G loss: 7.887870]\n",
      "[Epoch 0/200] [Batch 2785/3166] [D loss: -27.461960] [G loss: 1.207166]\n",
      "[Epoch 0/200] [Batch 2790/3166] [D loss: -25.731043] [G loss: 4.680087]\n",
      "[Epoch 0/200] [Batch 2795/3166] [D loss: -27.128735] [G loss: 4.385472]\n",
      "[Epoch 0/200] [Batch 2800/3166] [D loss: -27.740892] [G loss: 3.236350]\n",
      "[Epoch 0/200] [Batch 2805/3166] [D loss: -27.630459] [G loss: 4.091147]\n",
      "[Epoch 0/200] [Batch 2810/3166] [D loss: -29.519920] [G loss: 16.099876]\n",
      "[Epoch 0/200] [Batch 2815/3166] [D loss: -27.264057] [G loss: 3.597208]\n",
      "[Epoch 0/200] [Batch 2820/3166] [D loss: -25.911682] [G loss: 7.409762]\n",
      "[Epoch 0/200] [Batch 2825/3166] [D loss: -28.540653] [G loss: -0.593587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 2830/3166] [D loss: -23.666855] [G loss: 4.573415]\n",
      "[Epoch 0/200] [Batch 2835/3166] [D loss: -25.311363] [G loss: 1.727398]\n",
      "[Epoch 0/200] [Batch 2840/3166] [D loss: -25.554684] [G loss: 6.559539]\n",
      "[Epoch 0/200] [Batch 2845/3166] [D loss: -27.171156] [G loss: 0.753862]\n",
      "[Epoch 0/200] [Batch 2850/3166] [D loss: -24.895824] [G loss: 3.494643]\n",
      "[Epoch 0/200] [Batch 2855/3166] [D loss: -25.830433] [G loss: 8.722315]\n",
      "[Epoch 0/200] [Batch 2860/3166] [D loss: -27.626846] [G loss: 6.069223]\n",
      "[Epoch 0/200] [Batch 2865/3166] [D loss: -30.222885] [G loss: 10.100498]\n",
      "[Epoch 0/200] [Batch 2870/3166] [D loss: -24.597725] [G loss: 2.273001]\n",
      "[Epoch 0/200] [Batch 2875/3166] [D loss: -26.047409] [G loss: -1.760480]\n",
      "[Epoch 0/200] [Batch 2880/3166] [D loss: -27.757332] [G loss: 5.775844]\n",
      "[Epoch 0/200] [Batch 2885/3166] [D loss: -24.726669] [G loss: 11.920142]\n",
      "[Epoch 0/200] [Batch 2890/3166] [D loss: -25.501385] [G loss: 7.420437]\n",
      "[Epoch 0/200] [Batch 2895/3166] [D loss: -25.760477] [G loss: 5.234124]\n",
      "[Epoch 0/200] [Batch 2900/3166] [D loss: -26.500404] [G loss: 1.265831]\n",
      "[Epoch 0/200] [Batch 2905/3166] [D loss: -23.839241] [G loss: 2.690552]\n",
      "[Epoch 0/200] [Batch 2910/3166] [D loss: -27.995548] [G loss: 9.596569]\n",
      "[Epoch 0/200] [Batch 2915/3166] [D loss: -30.306561] [G loss: 4.427067]\n",
      "[Epoch 0/200] [Batch 2920/3166] [D loss: -25.570992] [G loss: 4.814815]\n",
      "[Epoch 0/200] [Batch 2925/3166] [D loss: -25.574448] [G loss: 4.539371]\n",
      "[Epoch 0/200] [Batch 2930/3166] [D loss: -25.072069] [G loss: 7.994850]\n",
      "[Epoch 0/200] [Batch 2935/3166] [D loss: -29.249233] [G loss: 5.654426]\n",
      "[Epoch 0/200] [Batch 2940/3166] [D loss: -27.096931] [G loss: 2.969169]\n",
      "[Epoch 0/200] [Batch 2945/3166] [D loss: -27.135494] [G loss: 5.953429]\n",
      "[Epoch 0/200] [Batch 2950/3166] [D loss: -27.265646] [G loss: 5.281274]\n",
      "[Epoch 0/200] [Batch 2955/3166] [D loss: -23.492081] [G loss: 7.845077]\n",
      "[Epoch 0/200] [Batch 2960/3166] [D loss: -27.671562] [G loss: 6.722356]\n",
      "[Epoch 0/200] [Batch 2965/3166] [D loss: -27.148849] [G loss: 7.933627]\n",
      "[Epoch 0/200] [Batch 2970/3166] [D loss: -27.880445] [G loss: 13.784218]\n",
      "[Epoch 0/200] [Batch 2975/3166] [D loss: -25.056030] [G loss: 3.901926]\n",
      "[Epoch 0/200] [Batch 2980/3166] [D loss: -26.876059] [G loss: 4.605323]\n",
      "[Epoch 0/200] [Batch 2985/3166] [D loss: -27.581604] [G loss: 2.603671]\n",
      "[Epoch 0/200] [Batch 2990/3166] [D loss: -23.813335] [G loss: 7.339358]\n",
      "[Epoch 0/200] [Batch 2995/3166] [D loss: -24.285440] [G loss: 2.023599]\n",
      "[Epoch 0/200] [Batch 3000/3166] [D loss: -28.600868] [G loss: 1.314873]\n",
      "[Epoch 0/200] [Batch 3005/3166] [D loss: -29.091951] [G loss: 6.988520]\n",
      "[Epoch 0/200] [Batch 3010/3166] [D loss: -29.995258] [G loss: 8.395302]\n",
      "[Epoch 0/200] [Batch 3015/3166] [D loss: -25.371916] [G loss: 9.435835]\n",
      "[Epoch 0/200] [Batch 3020/3166] [D loss: -25.164614] [G loss: 9.908382]\n",
      "[Epoch 0/200] [Batch 3025/3166] [D loss: -28.301962] [G loss: 2.186502]\n",
      "[Epoch 0/200] [Batch 3030/3166] [D loss: -26.925400] [G loss: 7.925632]\n",
      "[Epoch 0/200] [Batch 3035/3166] [D loss: -26.018494] [G loss: 4.427205]\n",
      "[Epoch 0/200] [Batch 3040/3166] [D loss: -27.369822] [G loss: 6.649185]\n",
      "[Epoch 0/200] [Batch 3045/3166] [D loss: -24.929567] [G loss: 10.455256]\n",
      "[Epoch 0/200] [Batch 3050/3166] [D loss: -26.979244] [G loss: 7.072128]\n",
      "[Epoch 0/200] [Batch 3055/3166] [D loss: -26.801157] [G loss: 10.540178]\n",
      "[Epoch 0/200] [Batch 3060/3166] [D loss: -24.597122] [G loss: 3.674815]\n",
      "[Epoch 0/200] [Batch 3065/3166] [D loss: -26.548149] [G loss: 11.868824]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "best_loss = 100.\n",
    "batches_done = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.uniform(0, 1, (imgs.shape[0], latent_dim))))\n",
    "        #z = Variable(torch.randn(batch_size, latent_dim, 1, 1))\n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z)\n",
    "\n",
    "        # Real images\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        # Fake images\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        adjust_learning_rate(optimizer_D, epoch, lr_d)\n",
    "        adjust_learning_rate(optimizer_G, epoch, lr_g)\n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "            if abs(g_loss.item()) < best_loss:\n",
    "                torch.save(generator,'./RMSprop_wgan')\n",
    "                best_loss = abs(g_loss.item())\n",
    "                print('model save at loss = %f' % (g_loss.item()))\n",
    "            if batches_done % sample_interval == 0:\n",
    "                generator.eval()\n",
    "                save_image(fake_imgs.data[:9], \"images8/%d.png\" % batches_done, nrow=3, normalize=True)\n",
    "                \n",
    "            batches_done += n_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1d0837442e55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mqq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msave\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msave_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"done_3/%03d.png\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    generator.eval()\n",
    "    qq = Variable(Tensor(np.random.normal(0, 1, (9, latent_dim))))\n",
    "    save = generator(qq)\n",
    "    save_image(save.data[:9], \"done_3/%03d.png\" % i, nrow=3, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at C:\\w\\1\\s\\tmp_conda_3.6_045031\\conda\\conda-bld\\pytorch_1565412750030\\work\\torch/csrc/generic/serialization.cpp:23",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-38251534bd0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./RMSprop_wgan'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dl\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dl\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[1;34m(f, mode, body)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dl\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dl\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m         \u001b[0mserialized_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at C:\\w\\1\\s\\tmp_conda_3.6_045031\\conda\\conda-bld\\pytorch_1565412750030\\work\\torch/csrc/generic/serialization.cpp:23"
     ]
    }
   ],
   "source": [
    "torch.save(generator,'./RMSprop_wgan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
